## Quantitative table

![](/images/quantitative_table.png)

For each training data, e.g. MNIST with training size $P$ 100, we show in the table, at which depth $L$ the dense model $f=0.5$ performed the best (compared to other depths with $f=0.5$). This depth $L$ and the generalization accuracy at this depth are shown in the first column "Dense - best acc.". We then a find a sparse model with a comparable performance, and show its $L$, $f$, and generalization accuracy in the second column "Sparse - equiv. acc.". We show that for all datasets, i.e. MNIST and CIFAR10 and all different training set sizes that we tested, the depth of the sparse model that performed comparably to the dense model is at least a half of that of the dense counterpart (compare $L$ under the columns "Dense - best acc." and  "Sparse - equiv. acc.") We check the performance of the dense model at the depth at which the sparse model performed comparably to the best dense model. This is to quantify the performance gain we get from sparsity at that given depth (compare the columns "Sparse - equiv. acc." and "Dense - same L"). The table shows there is always a performance gain from sparsity, and this gain is greater when $P$ is smaller or when the task is harder, e.g. CIFAR10. For each setup, we did 10 trials with randomly sampled training sets. We show the standard deviation of an accuracy with the $\pm$ notation. In short, it is quantitatively clear that sparse kernel requires a smaller depth, and hence less kernel compositions to reach the performance level observed in the deep dense model.
